# Gabriel's Data Engineering Portfolio

Building reliable, scalable data pipelines using modern tools and best practices.

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-4169E1?logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![Airflow](https://img.shields.io/badge/Airflow-017CEE?logo=apache-airflow&logoColor=white)](https://airflow.apache.org/)
[![dbt](https://img.shields.io/badge/dbt-FF694B?logo=dbt&logoColor=white)](https://www.getdbt.com/)
[![Spark](https://img.shields.io/badge/Spark-E25A1C?logo=apache-spark&logoColor=white)](https://spark.apache.org/)

## Overview

This repository showcases my hands-on data engineering work, starting from ingestion and orchestration up to production-grade batch and streaming pipelines. 
Projects are inspired by structured learning (e.g., Data Engineering Zoomcamp) but extended with my own experiments, production considerations, and optimizations.

Key themes:
- Containerized workflows (Docker & Compose)
- Data ingestion & transformation
- Workflow orchestration & scheduling
- Analytics engineering & modeling
- Data warehousing & ELT
- Batch & stream processing
- Testing, monitoring, & observability

## Project Structure
